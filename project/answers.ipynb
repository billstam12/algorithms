{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7873c6d-e515-45ec-ad62-7c72dfeb6c0f",
   "metadata": {},
   "source": [
    "### ULTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da20ac3-f3fe-4bf0-93cd-924c76ad7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from time import time\n",
    "import itertools\n",
    "from itertools import islice, chain\n",
    "from struct import *\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import errno\n",
    "import math\n",
    "\n",
    "# import pickle\n",
    "import dill as pickle \n",
    "import gffutils\n",
    "import pysam\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# from modules import create_splice_graph as splice_graph\n",
    "# from modules import graph_chainer \n",
    "\n",
    "from uLTRA.modules import create_augmented_gene as augmented_gene \n",
    "from uLTRA.modules import mem_wrapper \n",
    "from uLTRA.modules import colinear_solver \n",
    "from uLTRA.modules import help_functions\n",
    "from uLTRA.modules import classify_read_with_mams\n",
    "from uLTRA.modules import classify_alignment2\n",
    "from uLTRA.modules import sam_output\n",
    "from uLTRA.modules import align\n",
    "from uLTRA.modules import prefilter_genomic_reads\n",
    "\n",
    "from new_modules import functions\n",
    "from new_modules import evaluate_exons\n",
    "from new_modules import evaluate_splice_annotations\n",
    "from new_modules import get_diff_loc_reads\n",
    "from uLTRA.evaluation import plot_correctness_per_exon_size\n",
    "from uLTRA.evaluation import plots\n",
    "from uLTRA.evaluation import venn_diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6c1fd5-9e92-4fba-aade-1848dd2652cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "def output_final_alignments(ultra_alignments_path, path_indexed_aligned, path_unindexed_aligned):\n",
    "    # read in all reads from alternative aligner that was also mapped by uLTRA\n",
    "    \n",
    "    alt_alignments_file = pysam.AlignmentFile(path_indexed_aligned, \"r\", check_sq=False)\n",
    "    alt_alignments = { read.query_name : read for read in alt_alignments_file.fetch(until_eof=True) if not read.is_secondary }\n",
    "\n",
    "\n",
    "    alignment_infile = pysam.AlignmentFile( ultra_alignments_path, \"r\" )\n",
    "    tmp_merged_outfile = pysam.AlignmentFile( ultra_alignments_path.decode()+ 'tmp', \"w\", template= alignment_infile)\n",
    "    replaced_unaligned_cntr = 0\n",
    "    tot_counter = 0\n",
    "    scoring_dict = defaultdict(int)\n",
    "    for read in alignment_infile.fetch():\n",
    "        if not read.is_secondary:\n",
    "            tot_counter += 1\n",
    "\n",
    "        if read.query_name in alt_alignments:\n",
    "            if read.flag == 4:\n",
    "                read = alt_alignments[ read.query_name ] # replace unmapped uLTRA read with alternative alignment if mapped\n",
    "                replaced_unaligned_cntr += 1\n",
    "            elif not read.is_secondary: \n",
    "                ultra_scoring_diff, classification = check_alignment_fit(read,  alt_alignments[ read.query_name ])\n",
    "                scoring_dict[ultra_scoring_diff] += 1\n",
    "                if ultra_scoring_diff < 0:\n",
    "                    read = alt_alignments[ read.query_name ] # replace uLTRA read with alternative alignment if better fit\n",
    "\n",
    "        tmp_merged_outfile.write(read)\n",
    "    alignment_infile.close()\n",
    "    # path_genomic_aligned = os.path.join(args.outfolder, \"unindexed.sam\")\n",
    "\n",
    "    # add all reads that we did not attempt to align with uLTRA\n",
    "    # these reads had a primary alignment to unindexed regions by other pre-processing aligner (minimap2 as of now)\n",
    "    not_attempted_cntr = 0\n",
    "    unindexed = pysam.AlignmentFile(path_unindexed_aligned, \"r\")\n",
    "    for read in unindexed.fetch():\n",
    "        tmp_merged_outfile.write(read)\n",
    "        if not read.is_secondary: \n",
    "            not_attempted_cntr += 1\n",
    "    unindexed.close()\n",
    "    tmp_merged_outfile.close()\n",
    "    print(\"{0} reads were not attempted to be aligned with ultra, instead alternative aligner was used.\".format(not_attempted_cntr))\n",
    "    print(\"{0} reads with primary alignments were replaced with alternative aligner because they were unaligned with uLTRA.\".format(replaced_unaligned_cntr))\n",
    "    print(\"{0} primary alignments had best fit with uLTRA.\".format(sum([v for k,v in scoring_dict.items() if k > 0])))\n",
    "    print(\"{0} primary alignments had equal fit.\".format(scoring_dict[0]))\n",
    "    print(\"{0} primary alignments had best fit with alternative aligner.\".format(sum([v for k,v in scoring_dict.items() if k < 0])))\n",
    "\n",
    "    bin_boundaries = [-2**32, -100,-50,-20,-10,-5,-4,-3,-2,-1, 0, 1, 2, 3, 4, 5, 10, 20, 50, 100, 2**32]\n",
    "    n = len(bin_boundaries)\n",
    "    counts = [0]*n #{ (b_l, b_u) : 0 for b_l, b_u in zip(bin_boundaries[:-1], bin_boundaries[1:])}\n",
    "    start_next = 0\n",
    "    for k in sorted(scoring_dict.keys()):\n",
    "        for i in range(start_next, n):\n",
    "            b = bin_boundaries[i]\n",
    "        # for i, b in enumerate(bin_boundaries):\n",
    "            if k < b:\n",
    "                counts[i] += scoring_dict[k]\n",
    "            else:\n",
    "                start_next = i\n",
    "\n",
    "\n",
    "    print(\"Table of score-difference between alignment methods (negative number: alternative aligner better fit, positive number is uLTRA better fit)\")\n",
    "    print(\"Score is calculated as sum(identities) - sum(ins, del, subs)\")\n",
    "    print(\"Format: Score difference: Number of primary alignments \")\n",
    "    for i in range(len(counts)-1):\n",
    "        print(\"[{0} - {1}): {2}\".format(bin_boundaries[i],bin_boundaries[i+1], counts[i+1] - counts[i]))\n",
    "    # print(\"{0} read with primary alignments aligned with uLTRA.\".format(tot_counter - replaced_unaligned_cntr - replaced_fit_cntr))\n",
    "\n",
    "    shutil.move(ultra_alignments_path.decode()+ 'tmp', ultra_alignments_path)\n",
    "\n",
    "def batch(dictionary, size, batch_type):\n",
    "    # if batch_type == 'nt':\n",
    "    #     total_nt = sum([len(seq) for seq in dictionary.values() ])\n",
    "    batches = []\n",
    "    sub_dict = {}\n",
    "    curr_nt_count = 0\n",
    "    for i, (acc, seq) in enumerate(dictionary.items()):\n",
    "        curr_nt_count += len(seq)\n",
    "        if curr_nt_count >= size:\n",
    "            sub_dict[acc] = seq\n",
    "            batches.append(sub_dict)\n",
    "            sub_dict = {}\n",
    "            curr_nt_count = 0\n",
    "        else:\n",
    "            sub_dict[acc] = seq\n",
    "\n",
    "    if curr_nt_count/size != 0:\n",
    "        sub_dict[acc] = seq\n",
    "        batches.append(sub_dict)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "\n",
    "def load_reference(args):\n",
    "    refs = {acc : seq for acc, (seq, _) in help_functions.readfq(open(args.ref,\"r\"))}\n",
    "    refs_lengths = { acc : len(seq) for acc, seq in refs.items()} \n",
    "    return refs, refs_lengths\n",
    "\n",
    "def prep_splicing(args, refs_lengths):\n",
    "    if args.index:\n",
    "        index_folder = args.index\n",
    "        help_functions.mkdir_p(index_folder)\n",
    "    else:\n",
    "        index_folder = args.outfolder\n",
    "\n",
    "    database = os.path.join(index_folder,'database.db')\n",
    "\n",
    "    if os.path.isfile(database):\n",
    "        print(\"Database found in directory using this one.\")\n",
    "        print(\"If you want to recreate the database, please remove the file: {0}\".format(database))\n",
    "        print()\n",
    "        db = gffutils.FeatureDB(database, keep_order=True)\n",
    "        # sys.exit()\n",
    "    elif not args.disable_infer:\n",
    "        db = gffutils.create_db(args.gtf, dbfn=database, force=True, keep_order=True, merge_strategy='merge', \n",
    "                                sort_attribute_values=True)\n",
    "        db = gffutils.FeatureDB(database, keep_order=True)\n",
    "    else:\n",
    "        db = gffutils.create_db(args.gtf, dbfn=database, force=True, keep_order=True, merge_strategy='merge', \n",
    "                                sort_attribute_values=True, disable_infer_genes=True, disable_infer_transcripts=True)\n",
    "        db = gffutils.FeatureDB(database, keep_order=True)\n",
    "\n",
    "    \n",
    "    segment_to_ref, parts_to_segments, splices_to_transcripts, \\\n",
    "    transcripts_to_splices, all_splice_pairs_annotations, \\\n",
    "    all_splice_sites_annotations, segment_id_to_choordinates, \\\n",
    "    segment_to_gene, gene_to_small_segments, flank_choordinates, \\\n",
    "    max_intron_chr, exon_choordinates_to_id, chr_to_id, id_to_chr = augmented_gene.create_graph_from_exon_parts(db, args.flank_size, args.small_exon_threshold, args.min_segm, refs_lengths)\n",
    "\n",
    "    # dump to pickle here! Both graph and reference seqs\n",
    "    # help_functions.pickle_dump(args, genes_to_ref, 'genes_to_ref.pickle')\n",
    "    help_functions.pickle_dump(index_folder, segment_to_ref, 'segment_to_ref.pickle')\n",
    "    help_functions.pickle_dump(index_folder, splices_to_transcripts, 'splices_to_transcripts.pickle')\n",
    "    help_functions.pickle_dump(index_folder, transcripts_to_splices, 'transcripts_to_splices.pickle')\n",
    "    help_functions.pickle_dump(index_folder, parts_to_segments, 'parts_to_segments.pickle')\n",
    "    help_functions.pickle_dump(index_folder, all_splice_pairs_annotations, 'all_splice_pairs_annotations.pickle')\n",
    "    help_functions.pickle_dump(index_folder, all_splice_sites_annotations, 'all_splice_sites_annotations.pickle')\n",
    "    help_functions.pickle_dump(index_folder, segment_id_to_choordinates, 'segment_id_to_choordinates.pickle')\n",
    "    help_functions.pickle_dump(index_folder, segment_to_gene, 'segment_to_gene.pickle')\n",
    "    help_functions.pickle_dump(index_folder, gene_to_small_segments, 'gene_to_small_segments.pickle')\n",
    "    help_functions.pickle_dump(index_folder, flank_choordinates, 'flank_choordinates.pickle')\n",
    "    help_functions.pickle_dump(index_folder, max_intron_chr, 'max_intron_chr.pickle')\n",
    "    help_functions.pickle_dump(index_folder, exon_choordinates_to_id, 'exon_choordinates_to_id.pickle')\n",
    "    help_functions.pickle_dump(index_folder, chr_to_id, 'chr_to_id.pickle')\n",
    "    help_functions.pickle_dump(index_folder, id_to_chr, 'id_to_chr.pickle')\n",
    "\n",
    "    \n",
    "def check_alignment_fit(aln_ultra, aln_other):\n",
    "    \"\"\"\n",
    "        returns: \n",
    "        1. the differnce in scoring is positive if uLTRA better\n",
    "        2. the classification obtained by uLTRA\n",
    "    \"\"\"\n",
    "    diffs = {1,2,8} # cigar IDs for INS, DEL, SUBS\n",
    "    matches_ultra = sum([length for type_, length in aln_ultra.cigartuples if type_ == 7])\n",
    "    diffs_ultra = sum([length for type_, length in aln_ultra.cigartuples if type_ in diffs]) \n",
    "    matches_other = sum([length for type_, length in aln_other.cigartuples if type_ == 7])\n",
    "    diffs_other = sum([length for type_, length in aln_other.cigartuples if type_ in diffs]) \n",
    "    # print(matches_ultra, diffs_ultra, matches_other, diffs_other, matches_other - diffs_other <= matches_ultra - diffs_ultra)\n",
    "    # return matches_other - diffs_other <= matches_ultra - diffs_ultra\n",
    "    return (matches_ultra - diffs_ultra) - (matches_other - diffs_other), aln_ultra.get_tag('XC')\n",
    "\n",
    "def prep_seqs(args, refs, refs_lengths):\n",
    "    if args.index:\n",
    "        index_folder = args.index\n",
    "    else:\n",
    "        index_folder = args.outfolder\n",
    "\n",
    "    parts_to_segments = help_functions.pickle_load( os.path.join(index_folder, 'parts_to_segments.pickle') )\n",
    "    segment_id_to_choordinates = help_functions.pickle_load( os.path.join(index_folder, 'segment_id_to_choordinates.pickle') )\n",
    "    segment_to_ref = help_functions.pickle_load( os.path.join(index_folder, 'segment_to_ref.pickle') )\n",
    "    flank_choordinates = help_functions.pickle_load( os.path.join(index_folder, 'flank_choordinates.pickle') )\n",
    "    exon_choordinates_to_id = help_functions.pickle_load( os.path.join(index_folder, 'exon_choordinates_to_id.pickle') )\n",
    "    chr_to_id = help_functions.pickle_load( os.path.join(index_folder, 'chr_to_id.pickle') )\n",
    "    id_to_chr = help_functions.pickle_load( os.path.join(index_folder, 'id_to_chr.pickle') )\n",
    "\n",
    "    # for chr_id in id_to_chr:\n",
    "    #     print(chr_id, id_to_chr[chr_id])\n",
    "\n",
    "    # tiling_parts_to_segments = help_functions.pickle_load( os.path.join(args.outfolder, 'tiling_parts_to_segments.pickle') )\n",
    "    # tiling_segment_id_to_choordinates = help_functions.pickle_load( os.path.join(args.outfolder, 'tiling_segment_id_to_choordinates.pickle') )\n",
    "    # tiling_segment_to_ref = help_functions.pickle_load( os.path.join(args.outfolder, 'tiling_segment_to_ref.pickle') )\n",
    "    \n",
    "    print( \"Number of ref seqs in gff:\", len(parts_to_segments.keys()))\n",
    "\n",
    "    refs_id = {}\n",
    "\n",
    "    not_in_annot = set()\n",
    "    for acc, seq in refs.items():\n",
    "        if acc not in chr_to_id:\n",
    "            not_in_annot.add(acc)\n",
    "        else:\n",
    "            acc_id = chr_to_id[acc]\n",
    "            refs_id[acc_id] = seq\n",
    "\n",
    "    refs_id_lengths = { acc_id : len(seq) for acc_id, seq in refs_id.items()} \n",
    "    help_functions.pickle_dump(index_folder, refs_id_lengths, 'refs_id_lengths.pickle')\n",
    "    help_functions.pickle_dump(index_folder, refs_lengths, 'refs_lengths.pickle')\n",
    "\n",
    "    print( \"Number of ref seqs in fasta:\", len(refs.keys()))\n",
    "\n",
    "    not_in_ref = set(chr_to_id.keys()) - set(refs.keys())\n",
    "    if not_in_ref:\n",
    "        print(\"Warning: Detected {0} sequences that are in annotation but not in reference fasta. Using only sequences present in fasta. The following sequences cannot be detected in reference fasta:\\n\".format(len(not_in_ref)))\n",
    "        for s in not_in_ref:\n",
    "            print(s)\n",
    "\n",
    "    if not_in_annot:\n",
    "        print(\"Warning: Detected {0} sequences in reference fasta that are not in annotation:\\n\".format(len(not_in_annot)))\n",
    "        for s in not_in_annot:\n",
    "            print(s, \"with length:{0}\".format(len(refs[s])))\n",
    "    # ref_part_sequences, ref_flank_sequences = augmented_gene.get_part_sequences_from_choordinates(parts_to_segments, flank_choordinates, refs_id)\n",
    "    ref_part_sequences = augmented_gene.get_sequences_from_choordinates(parts_to_segments, refs_id)\n",
    "    ref_flank_sequences = augmented_gene.get_sequences_from_choordinates(flank_choordinates, refs_id)\n",
    "\n",
    "    if not args.use_NAM_seeds: # not using NAM seeds\n",
    "        augmented_gene.mask_abundant_kmers(ref_part_sequences, args.min_mem, args.mask_threshold)\n",
    "        augmented_gene.mask_abundant_kmers(ref_flank_sequences, args.min_mem, args.mask_threshold)\n",
    "\n",
    "    # print([unpack('LLL',t) for t in ref_flank_sequences.keys()])\n",
    "    ref_part_sequences = help_functions.update_nested(ref_part_sequences, ref_flank_sequences)\n",
    "    ref_segment_sequences = augmented_gene.get_sequences_from_choordinates(segment_id_to_choordinates, refs_id)\n",
    "    # ref_flank_sequences = augmented_gene.get_sequences_from_choordinates(flank_choordinates, refs_id)\n",
    "    ref_exon_sequences = augmented_gene.get_sequences_from_choordinates(exon_choordinates_to_id, refs_id)\n",
    "    help_functions.pickle_dump(index_folder, segment_id_to_choordinates, 'segment_id_to_choordinates.pickle')\n",
    "    help_functions.pickle_dump(index_folder, ref_part_sequences, 'ref_part_sequences.pickle')\n",
    "    help_functions.pickle_dump(index_folder, ref_segment_sequences, 'ref_segment_sequences.pickle')\n",
    "    help_functions.pickle_dump(index_folder, ref_flank_sequences, 'ref_flank_sequences.pickle')\n",
    "    help_functions.pickle_dump(index_folder, ref_exon_sequences, 'ref_exon_sequences.pickle')\n",
    "\n",
    "def align_reads(args):\n",
    "    stats = {}\n",
    "   \n",
    "    stats[\"dataset\"] = args.name\n",
    "    \n",
    "    \n",
    "    if args.nr_cores > 1:\n",
    "        if(not mp.get_context()):\n",
    "            mp.set_start_method('spawn')\n",
    "        print(mp.get_context())\n",
    "        print(\"Environment set:\", mp.get_context())\n",
    "        print(\"Using {0} cores.\".format(args.nr_cores))\n",
    "\n",
    "    if args.index:\n",
    "        if os.path.isdir(args.index):\n",
    "            index_folder = args.index\n",
    "        else:\n",
    "            print(\"The index folder specified for alignment is not found. You specified: \", args.index )\n",
    "            print(\"Build  the index to this folder, or specify another forder where the index has been built.\" )\n",
    "            sys.exit()\n",
    "    else:\n",
    "        index_folder = args.outfolder\n",
    "\n",
    "    # topological_sorts = help_functions.pickle_load( os.path.join(args.outfolder, 'top_sorts.pickle') )\n",
    "    # path_covers = help_functions.pickle_load( os.path.join(args.outfolder, 'paths.pickle') )\n",
    "\n",
    "    ref_part_sequences = help_functions.pickle_load( os.path.join(index_folder, 'ref_part_sequences.pickle') )\n",
    "    refs_id_lengths = help_functions.pickle_load( os.path.join(index_folder, 'refs_id_lengths.pickle') )\n",
    "    refs_lengths = help_functions.pickle_load( os.path.join(index_folder, 'refs_lengths.pickle') )\n",
    "\n",
    "    reads_start = args.reads\n",
    "    # here reads change\n",
    "    if not args.disable_mm2:\n",
    "        print(\"Filtering reads aligned to unindexed regions with minimap2 \")\n",
    "        nr_reads_to_ignore, path_reads_to_align = prefilter_genomic_reads.main(ref_part_sequences, args.ref, args.reads,\n",
    "                                                                               args.outfolder, index_folder, args.nr_cores,\n",
    "                                                                               args.genomic_frac, args.mm2_ksize, args.minimap_path)\n",
    "        args.reads = path_reads_to_align\n",
    "        print(\"Done filtering. Reads filtered:{0}\".format(nr_reads_to_ignore))\n",
    "    ref_path = os.path.join(args.outfolder, \"refs_sequences.fa\")\n",
    "    refs_file = open(ref_path, 'w') #open(os.path.join(outfolder, \"refs_sequences_tmp.fa\"), \"w\")\n",
    "    for sequence_id, seq  in ref_part_sequences.items():\n",
    "        chr_id, start, stop = unpack('LLL',sequence_id)\n",
    "        # for (start,stop), seq  in ref_part_sequences[chr_id].items():\n",
    "        refs_file.write(\">{0}\\n{1}\\n\".format(str(chr_id) + str(\"^\") + str(start) + \"^\" + str(stop), seq))\n",
    "    refs_file.close()\n",
    "\n",
    "    del ref_part_sequences\n",
    "\n",
    "    ######### FIND MEMS WITH MUMMER #############\n",
    "    #############################################\n",
    "    #############################################\n",
    "\n",
    "    mummer_start = time()\n",
    "    if args.use_NAM_seeds:\n",
    "        print(\"Processing reads for MEM finding\")\n",
    "        reads_tmp = open(os.path.join(args.outfolder, 'reads_tmp.fq'), 'w')\n",
    "        for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r')):\n",
    "            # print(seq)\n",
    "            # print(help_functions.remove_read_polyA_ends(seq, args.reduce_read_ployA))\n",
    "            reads_tmp.write('>{0}\\n{1}\\n'.format(acc, help_functions.remove_read_polyA_ends(seq, args.reduce_read_ployA, 5)))\n",
    "        reads_tmp.close()\n",
    "        args.reads_tmp = reads_tmp.name\n",
    "        mem_wrapper.find_nams_strobemap(args.outfolder, args.reads_tmp, ref_path, args.outfolder, args.nr_cores, args.min_mem)\n",
    "    else: # Use slaMEM\n",
    "        if args.nr_cores == 1:\n",
    "            print(\"Processing reads for MEM finding\")\n",
    "            reads_tmp = open(os.path.join(args.outfolder, 'reads_tmp.fq'), 'w')\n",
    "            for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r')):\n",
    "                # print(seq)\n",
    "                # print(help_functions.remove_read_polyA_ends(seq, args.reduce_read_ployA))\n",
    "                reads_tmp.write('>{0}\\n{1}\\n'.format(acc, help_functions.remove_read_polyA_ends(seq, args.reduce_read_ployA, 5)))\n",
    "            reads_tmp.close()\n",
    "            args.reads_tmp = reads_tmp.name\n",
    "            print(\"Finished processing reads for MEM finding \")\n",
    "\n",
    "            mummer_out_path =  os.path.join( args.outfolder, \"seeds_batch_-1.txt\" )\n",
    "            print(\"Running MEM finding forward\") \n",
    "            mem_wrapper.find_mems_slamem(args.slamem_path, args.mummer_path, args.outfolder, args.reads_tmp, ref_path, mummer_out_path, args.min_mem)\n",
    "            print(\"Completed MEM finding forward\")\n",
    "\n",
    "            print(\"Processing reverse complement reads for MEM finding\")\n",
    "            reads_rc = open(os.path.join(args.outfolder, 'reads_rc.fq'), 'w')\n",
    "            for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r')):\n",
    "                # print(help_functions.reverse_complement(seq))\n",
    "                # print(help_functions.remove_read_polyA_ends(help_functions.reverse_complement(seq), args.reduce_read_ployA))\n",
    "                reads_rc.write('>{0}\\n{1}\\n'.format(acc, help_functions.reverse_complement(help_functions.remove_read_polyA_ends(seq, args.reduce_read_ployA, 5))))\n",
    "            reads_rc.close()\n",
    "            args.reads_rc = reads_rc.name\n",
    "            print(\"Finished processing reverse complement reads for MEM finding\")\n",
    "\n",
    "            mummer_out_path =  os.path.join(args.outfolder, \"seeds_batch_-1_rc.txt\" )\n",
    "            print(\"Running MEM finding reverse\")\n",
    "            mem_wrapper.find_mems_slamem(args.slamem_path, args.mummer_path, args.outfolder, args.reads_rc, ref_path, mummer_out_path, args.min_mem)\n",
    "            print(\"Completed MEM finding reverse\")\n",
    "        \n",
    "        else: # multiprocess with slaMEM\n",
    "            reads = { acc : seq for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r'))}\n",
    "            total_nt = sum([len(seq) for seq in reads.values() ])\n",
    "            batch_size = int(total_nt/int(args.nr_cores) + 1)\n",
    "            print(\"batch nt:\", batch_size, \"total_nt:\", total_nt)\n",
    "            read_batches = batch(reads, batch_size, 'nt')\n",
    "            \n",
    "            #### TMP remove not to call mummer repeatedly when bugfixing #### \n",
    "            \n",
    "            batch_args = []\n",
    "            for i, read_batch_dict in enumerate(read_batches):\n",
    "                print(len(read_batch_dict))\n",
    "                read_batch_temp_file = open(os.path.join(args.outfolder, \"reads_batch_{0}.fa\".format(i)), \"w\")\n",
    "                read_batch_temp_file_rc = open(os.path.join(args.outfolder, \"reads_batch_{0}_rc.fa\".format(i) ), \"w\")\n",
    "                for acc, seq in read_batch_dict.items():\n",
    "                    read_batch_temp_file.write('>{0}\\n{1}\\n'.format(acc, help_functions.remove_read_polyA_ends(seq, args.reduce_read_ployA, 5)))\n",
    "                read_batch_temp_file.close()\n",
    "\n",
    "                for acc, seq in read_batch_dict.items():\n",
    "                    read_batch_temp_file_rc.write('>{0}\\n{1}\\n'.format(acc, help_functions.reverse_complement(help_functions.remove_read_polyA_ends(seq, args.reduce_read_ployA, 5))))\n",
    "                read_batch_temp_file_rc.close()\n",
    "                \n",
    "                read_batch = read_batch_temp_file.name\n",
    "                read_batch_rc = read_batch_temp_file_rc.name\n",
    "                mummer_batch_out_path =  os.path.join( args.outfolder, \"seeds_batch_{0}.txt\".format(i) )\n",
    "                mummer_batch_out_path_rc =  os.path.join(args.outfolder, \"seeds_batch_{0}_rc.txt\".format(i) )\n",
    "                batch_args.append( (args.slamem_path, args.mummer_path, args.outfolder, read_batch, ref_path, mummer_batch_out_path, args.min_mem ) )\n",
    "                batch_args.append( (args.slamem_path, args.mummer_path, args.outfolder, read_batch_rc, ref_path, mummer_batch_out_path_rc, args.min_mem ) )\n",
    "\n",
    "            pool = Pool(processes=int(args.nr_cores))\n",
    "            results = pool.starmap(mem_wrapper.find_mems_slamem, batch_args)\n",
    "            pool.close()\n",
    "            pool.join() \n",
    "\n",
    "\n",
    "            ####################################################################\n",
    "\n",
    "\n",
    "        print(\"Time for slaMEM to find mems:{0} seconds.\".format(time()-mummer_start))\n",
    "    stats[\"mem_time\"] = time()-mummer_start\n",
    "    #############################################\n",
    "    #############################################\n",
    "    #############################################\n",
    "\n",
    "\n",
    "    print(\"Starting aligning reads.\")\n",
    "    if args.use_NAM_seeds:\n",
    "        if args.nr_cores == 1:\n",
    "            reads = { acc : seq for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r'))}\n",
    "            classifications, alignment_outfile_name = align.align_single(reads, refs_id_lengths, args, -1)\n",
    "        else:\n",
    "            # OrderedDict # dicts are ordered from python v3.6 and above. \n",
    "            # One can use OrderedDict for compatibility with python v 3.4-3.5\n",
    "            reads = {acc : seq for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r'))}\n",
    "            # batch reads and mems up: divide reads by  nr_cores to get batch size\n",
    "            # then write to separate SAM-files with a batch index, \n",
    "            # finally merge sam file by simple cat in terminal \n",
    "            aligning_start = time()\n",
    "            # batch_size = int(len(reads)/int(args.nr_cores) + 1)\n",
    "            STROBEMAP_BATCH_SIZE=500000\n",
    "            read_batches = strobemap_batching(reads, STROBEMAP_BATCH_SIZE, int(args.nr_cores))\n",
    "            print('Nr reads:', len(reads), \"nr batches:\", len(read_batches), [len(b) for b in read_batches])\n",
    "            stats[\"reads\"] = len(reads)\n",
    "            classifications, alignment_outfiles = align.align_parallel(read_batches, refs_id_lengths, args)\n",
    "        \n",
    "            print(\"Time to align reads:{0} seconds.\".format(time()-aligning_start))\n",
    "            stats[\"align_time\"] = time()-aligning_start\n",
    "            # Combine samfiles produced from each batch\n",
    "            combine_start = time()\n",
    "            # print(refs_lengths)\n",
    "            alignment_outfile = pysam.AlignmentFile( os.path.join(args.outfolder, args.prefix+\".sam\"), \"w\", reference_names=list(refs_lengths.keys()), reference_lengths=list(refs_lengths.values()) ) #, template=samfile)\n",
    "\n",
    "            for f in alignment_outfiles:\n",
    "                samfile = pysam.AlignmentFile(f, \"r\")\n",
    "                for read in samfile.fetch():\n",
    "                    alignment_outfile.write(read)\n",
    "                samfile.close()\n",
    "\n",
    "            alignment_outfile.close()\n",
    "            alignment_outfile_name = alignment_outfile.filename\n",
    "            print(\"Time to merge SAM-files:{0} seconds.\".format(time() - combine_start))\n",
    "            stats[\"merge_time\"] = time()-combine_start\n",
    "\n",
    "    else: # Use slaMEM\n",
    "        if args.nr_cores == 1:\n",
    "            reads = { acc : seq for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r'))}\n",
    "            classifications, alignment_outfile_name = align.align_single(reads, refs_id_lengths, args, -1)\n",
    "        else:\n",
    "            # batch reads and mems up: divide reads by  nr_cores to get batch size\n",
    "            # then write to separate SAM-files with a batch index, \n",
    "            # finally merge sam file by simple cat in terminal \n",
    "            aligning_start = time()\n",
    "            batch_size = int(len(reads)/int(args.nr_cores) + 1)\n",
    "            # read_batches = batch(reads, batch_size)\n",
    "            print('Nr reads:', len(reads), \"nr batches:\", len(read_batches), [len(b) for b in read_batches])\n",
    "            stats[\"reads\"] = len(reads)\n",
    "            classifications, alignment_outfiles = align.align_parallel(read_batches, refs_id_lengths, args)\n",
    "        \n",
    "            print(\"Time to align reads:{0} seconds.\".format(time()-aligning_start))\n",
    "            stats[\"align_time\"] = time()-aligning_start\n",
    "\n",
    "            # Combine samfiles produced from each batch\n",
    "            combine_start = time()\n",
    "            # print(refs_lengths)\n",
    "            alignment_outfile = pysam.AlignmentFile( os.path.join(args.outfolder, args.prefix+\".sam\"), \"w\", reference_names=list(refs_lengths.keys()), reference_lengths=list(refs_lengths.values()) ) #, template=samfile)\n",
    "\n",
    "            for f in alignment_outfiles:\n",
    "                samfile = pysam.AlignmentFile(f, \"r\")\n",
    "                for read in samfile.fetch():\n",
    "                    alignment_outfile.write(read)\n",
    "                samfile.close()\n",
    "\n",
    "            alignment_outfile.close()\n",
    "            alignment_outfile_name = alignment_outfile.filename\n",
    "            print(\"Time to merge SAM-files:{0} seconds.\".format(time() - combine_start))\n",
    "            stats[\"merge_time\"] = time()-combine_start\n",
    "\n",
    "\n",
    "    # need to merge genomic/unindexed alignments with the uLTRA-aligned alignments\n",
    "    if not args.disable_mm2:\n",
    "        path_indexed_aligned = os.path.join(args.outfolder, \"indexed.sam\")\n",
    "        path_unindexed_aligned = os.path.join(args.outfolder, \"unindexed.sam\")\n",
    "        output_final_alignments(alignment_outfile_name, path_indexed_aligned, path_unindexed_aligned)\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    alignment_coverage = 0\n",
    "    for read_acc in reads:\n",
    "        if read_acc not in classifications:\n",
    "            # print(read_acc, \"did not meet the threshold\")\n",
    "            pass\n",
    "        elif classifications[read_acc][0] != 'FSM':\n",
    "            # print(read_acc, classifications[read_acc]) \n",
    "            pass\n",
    "        if read_acc in classifications:\n",
    "            alignment_coverage += classifications[read_acc][1]\n",
    "            if classifications[read_acc][1] < 1.0:\n",
    "                # print(read_acc, 'alignemnt coverage:', classifications[read_acc][1])\n",
    "                pass\n",
    "            counts[classifications[read_acc][0]] += 1\n",
    "        else:\n",
    "            counts['unaligned'] += 1\n",
    "\n",
    "\n",
    "    print(counts)\n",
    "    json.dump(functions.transform_categories(counts), open(os.path.join(args.outfolder, \"counts.json\"), \"w\"))\n",
    "    print(\"total alignment coverage:\", alignment_coverage)\n",
    "\n",
    "    if not args.keep_temporary_files:\n",
    "        print(\"Deleting temporary files...\")\n",
    "        seeds = glob.glob(os.path.join(args.outfolder, \"seeds_*\"))\n",
    "        mum = glob.glob(os.path.join(args.outfolder, \"mummer*\"))\n",
    "        sla = glob.glob(os.path.join(args.outfolder, \"slamem*\"))\n",
    "        reads_tmp = glob.glob(os.path.join(args.outfolder, \"reads_batch*\"))\n",
    "        minimap_tmp = glob.glob(os.path.join(args.outfolder, \"minimap2*\"))\n",
    "        ultra_tmp = glob.glob(os.path.join(args.outfolder, \"uLTRA_batch*\"))\n",
    "        \n",
    "        f1 = os.path.join(args.outfolder, \"reads_after_genomic_filtering.fasta\")\n",
    "        f2 = os.path.join(args.outfolder, \"indexed.sam\")\n",
    "        f3 = os.path.join(args.outfolder, \"unindexed.sam\")\n",
    "        f4 = os.path.join(args.outfolder, \"refs_sequences.fa\")\n",
    "        f5 = os.path.join(args.outfolder, \"refs_sequences.fa\")\n",
    "        f6 = os.path.join(args.outfolder, \"reads_rc.fq\")\n",
    "        f7 = os.path.join(args.outfolder, \"reads_tmp.fq\")\n",
    "        misc_files = [f1,f2,f3,f4,f5,f6,f7]\n",
    "        for f in seeds + mum + sla + reads_tmp + minimap_tmp + ultra_tmp+ misc_files:\n",
    "            if os.path.isfile(f):\n",
    "                os.remove(f)\n",
    "                print(\"removed:\", f)\n",
    "    print(\"Done.\")\n",
    "    # save stats\n",
    "    stats[\"total_time\"] = stats[\"align_time\"] + stats[\"mem_time\"] + stats[\"merge_time\"]\n",
    "    json.dump(stats, open(os.path.join(args.outfolder, \"stats.json\"), \"w\"))\n",
    "    args.reads = reads_start\n",
    "\n",
    "    return stats\n",
    "\n",
    "def initialize_dump(outfolder):\n",
    "    if os.path.exists(outfolder) and os.path.isdir(outfolder):\n",
    "        return;\n",
    "        #shutil.rmtree(outfolder)\n",
    "    help_functions.mkdir_p(outfolder)\n",
    "        \n",
    "def ultra(args):\n",
    "    # initialize dump folder\n",
    "    reads = os.path.join(args.outfolder, \"reads.sam\")\n",
    "    if (os.path.exists(reads)):\n",
    "        return json.load(open(os.path.join(args.outfolder, \"stats.json\"), \"r\"))\n",
    "    else:\n",
    "        initialize_dump(args.outfolder)\n",
    "        refs, refs_lengths = load_reference(args)\n",
    "        prep_splicing(args, refs_lengths)\n",
    "        prep_seqs(args, refs, refs_lengths)\n",
    "        return align_reads(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26653153-d215-413c-b149-a35ebb4908c1",
   "metadata": {},
   "source": [
    "### Minimap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d765cc-77cf-4ed1-8b9f-7b361bb5e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "output = \"output/test.sam\"\n",
    "\n",
    "# set minimap arguments based on supplementary data\n",
    "def set_minimap_args(args):\n",
    "    G = \"500k\" # Maximum gap on the reference (effective with -xsplice/--splice).\n",
    "    t = 4 # no of threads\n",
    "    k = 13 # kmer, set to 14 for ALZ\n",
    "    w = 5 # minimum window size set to none for ALZ\n",
    "    minimap_path = args.minimap_path\n",
    "\n",
    "    if(args.bed == None):\n",
    "        \n",
    "        if(\"ALZ\" in args.ref):\n",
    "            k = 14 # kmer, set to 14 for ALZ\n",
    "            t = 19 # no of threads\n",
    "            G = \"500k\" # Maximum gap on the reference (effective with -xsplice/--splice).\n",
    "            return [minimap_path, \"-a\", args.ref, args.reads, \n",
    "                               \"-k\", str(k), \"--eqx\", \"-t\", str(t), \n",
    "                               \"-ax\" ,\"splice\", \"-G\", G,  \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "        elif(\"SIRV\" in args.ref):\n",
    "            return [minimap_path, \"-a\", args.ref, args.reads, \n",
    "                               \"-k\", str(k), \"--eqx\", \"-t\", str(t), \n",
    "                               \"-ax\" ,\"splice\", \"-w\", str(w), \"-G\", G, \"--splice-flank=no\",\n",
    "                                \"--secondary=no\", \"-C\", \"5\", \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "        else:\n",
    "            return [minimap_path, \"-a\", args.ref, args.reads, \n",
    "                               \"-k\", str(k), \"--eqx\", \"-t\", str(t), \n",
    "                               \"-ax\" ,\"splice\", \"-w\", str(w), \"-G\", G, \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "    else:\n",
    "        if(\"ALZ\" in args.ref):\n",
    "            k = 14 # kmer, set to 14 for ALZ\n",
    "            t = 19 # no of threads\n",
    "            G = \"500k\" # Maximum gap on the reference (effective with -xsplice/--splice).\n",
    "            return [minimap_path, \"-a\", args.ref, args.reads, \"--junc-bed\", args.bed,\n",
    "                               \"-k\", str(k), \"--eqx\", \"-t\", str(t), \n",
    "                               \"-ax\" ,\"splice\", \"-G\", G,  \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "        elif(\"SIRV\" in args.ref):\n",
    "            return [minimap_path, \"-a\", args.ref, args.reads, \"--junc-bed\", args.bed,\n",
    "                               \"-k\", str(k), \"--eqx\", \"-t\", str(t), \n",
    "                               \"-ax\" ,\"splice\", \"-w\", str(w), \"-G\", G, \"--splice-flank=no\",\n",
    "                                \"--secondary=no\", \"-C\", \"5\", \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "        else:\n",
    "            return [minimap_path, \"-a\", args.ref, args.reads, \"--junc-bed\", args.bed,\n",
    "                               \"-k\", str(k), \"--eqx\", \"-t\", str(t), \n",
    "                               \"-ax\" ,\"splice\", \"-w\", str(w), \"-G\", G, \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "    \n",
    "\n",
    "def minimap2(args):\n",
    "    reads = os.path.join(args.outfolder, \"reads.sam\")\n",
    "    if (not os.path.exists(reads)):\n",
    "        stats = {}\n",
    "        stats[\"dataset\"] = args.name\n",
    "\n",
    "        initialize_dump(args.outfolder)\n",
    "        mm2_start = time()\n",
    "\n",
    "        subprocess.check_call(set_minimap_args(args), env = os.environ)\n",
    "        stats[\"total_time\"] = time() - mm2_start\n",
    "        json.dump(stats, open(os.path.join(args.outfolder, \"stats.json\"), \"w\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8f89b-6e9f-4e3a-a759-9d24feefc3be",
   "metadata": {},
   "source": [
    "### deSALT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cde73c1-0fc8-4f92-8140-0562d01aef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_desalt_args(args):\n",
    "    d = 10 \n",
    "    s = 2\n",
    "    l = 14\n",
    "    noncan = 9\n",
    "    max_intron_length = 500000\n",
    "    index_path = args.desalt_index\n",
    "    desalt_path = args.desalt_path\n",
    "    if(args.desalt_gtf == None):\n",
    "        if(\"SIRV\" in args.ref):\n",
    "            noncan = 4\n",
    "            max_intron_length = 200000\n",
    "            return [desalt_path, \"aln\", index_path, args.reads, \n",
    "                               \"-d\", str(d), \"-s\", str(s), \"-l\", str(l),\n",
    "                                \"--noncan\", str(noncan), \"--max-intron-len\", str(max_intron_length), \n",
    "                    \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "        else:\n",
    "            return [desalt_path, \"aln\", index_path, args.reads, \n",
    "                               \"-d\", str(d), \"-s\", str(s), \"-l\", str(l),\n",
    "                                \"--noncan\", str(noncan), \"--max-intron-len\", str(max_intron_length),\n",
    "                    \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "    else:\n",
    "        if(\"SIRV\" in args.ref):\n",
    "            noncan = 4\n",
    "            max_intron_length = 200000\n",
    "            return [desalt_path, \"aln\", index_path, args.reads, \n",
    "                               \"-d\", str(d), \"-s\", str(s), \"-l\", str(l),\n",
    "                                \"--noncan\", str(noncan), \"--max-intron-len\", str(max_intron_length),\n",
    "                                \"--gtf\", args.desalt_gtf,\n",
    "                                \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "        else:\n",
    "            return [desalt_path, \"aln\", index_path, args.reads, \n",
    "                               \"-d\", str(d), \"-s\", str(s), \"-l\", str(l),\n",
    "                                \"--noncan\", str(noncan), \"--max-intron-len\", str(max_intron_length),\n",
    "                                \"--gtf\", args.desalt_gtf,\n",
    "                                \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "\n",
    "def deSALT(args):\n",
    "    reads = os.path.join(args.outfolder, \"reads.sam\")\n",
    "    if (not os.path.exists(reads)):\n",
    "        stats = {}\n",
    "        stats[\"dataset\"] = args.name\n",
    "        initialize_dump(args.outfolder)\n",
    "        deSALT_start = time()\n",
    "        if not os.path.exists(args.desalt_index): \n",
    "            subprocess.check_call([args.desalt_path, \"index\",args.ref, args.desalt_index])\n",
    "            stats[\"index_time\"] = time() - deSALT_start\n",
    "        subprocess.check_call(set_desalt_args(args), env = os.environ)\n",
    "        stats[\"total_time\"] = time() - deSALT_start\n",
    "        json.dump(stats, open(os.path.join(args.outfolder, \"stats.json\"), \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc8cd6-119e-4700-bb11-66c1fe7daab1",
   "metadata": {},
   "source": [
    "### graphmap2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98eeb2a8-b5b9-4d69-855c-db47ee22e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_graphmap2_args(args):\n",
    "    graphmap_path = args.graphmap_path\n",
    "    if(args.gtf == None):\n",
    "        return [graphmap_path, \"align\",\n",
    "                               \"-d\",  args.reads, \"-r\", args.ref, \n",
    "                               \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "    else: \n",
    "        return [graphmap_path, \"align\", \"--gtf\", args.gtf,\n",
    "                               \"-d\",  args.reads, \"-r\", args.ref, \n",
    "                               \"-o\", os.path.join(args.outfolder, \"reads.sam\")]\n",
    "\n",
    "def graphmap2(args):\n",
    "    reads = os.path.join(args.outfolder, \"reads.sam\")\n",
    "    if (not os.path.exists(reads)):\n",
    "        stats = {}\n",
    "        stats[\"dataset\"] = args.name\n",
    "        initialize_dump(args.outfolder)\n",
    "        graphmap2_start = time()\n",
    "        subprocess.check_call(set_graphmap2_args(args), env = os.environ)\n",
    "        stats[\"total_time\"] = time() - graphmap2_start\n",
    "        json.dump(stats, open(os.path.join(args.outfolder, \"stats.json\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb3bc10-b020-432e-8d49-83acd077a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_modules.arguments import arguments\n",
    "\n",
    "def create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, name, tool_name, mm2, desalt_index):\n",
    "    args = arguments(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, name, tool_name, mm2, desalt_index)\n",
    "    s = args.reads\n",
    "    s = s[s.rindex('/')+1:]\n",
    "    s = s[:s.rindex('.')]\n",
    "    args.name = s\n",
    "    if args.ont:\n",
    "        args.min_mem = 17\n",
    "        args.min_acc = 0.6\n",
    "        args.mm2_ksize = 14\n",
    "        # args.alignment_threshold = 0.5\n",
    "    if args.isoseq:\n",
    "        args.min_mem = 20\n",
    "        args.min_acc = 0.8\n",
    "        # args.alignment_threshold = 0.5\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a28e6e-8f8b-4ceb-9492-7a0ff529611d",
   "metadata": {},
   "source": [
    "![Datasets Used](assets/images/table1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c72358-65f7-499e-aea2-37f49a755d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dict with the paths of each dataset\n",
    "dataset_dict = {\n",
    "    # \"test\" : {\n",
    "    #     \"ref\": \"data/genome/SIRV_isoforms_multi-fasta_170612a.fasta\" ,\n",
    "    #     \"reads\": \"uLTRA/test/reads.fa\" ,\n",
    "    #     \"gtf\": \"data/genome/annotations/SIRV_isoforms_Lot00141_multi-fasta-annotation_C_170612a.gtf\",\n",
    "    #     \"bed\":\"data/genome/annotations/bed/SIRV_isoforms_Lot00141_multi-fasta-annotation_C_170612a.bed\",\n",
    "    #     \"desalt_gtf\": \"data/genome/annotations/desalt_gtf/SIRV_isoforms_Lot00141_multi-fasta-annotation_C_170612a.gtf\",\n",
    "    #     \"ont\": False,\n",
    "    #     \"isoseq\": True,\n",
    "    #     \"disable_infer\": False,\n",
    "    #     \"desalt_index\":\"data/genome/indexes/sirv\",\n",
    "    # },\n",
    "    \"ENS\": {\"ref\": \"data/genome/GRCh38.p12.genome.fa\" ,\n",
    "            \"reads\": \"data/simulated/reads_ens.fa\" ,\n",
    "            \"gtf\": \"data/genome/annotations/gencode.v34.chr_patch_hapl_scaff.annotation.gtf\" ,\n",
    "            \"bed\": \"data/genome/annotations/bed/gencode.v34.annotation.bed\",\n",
    "            \"desalt_gtf\": \"data/genome/annotations/desalt_gtf/gencode.v34.annotation.bed\",\n",
    "            \"ont\": False,\n",
    "            \"isoseq\": False,\n",
    "            \"disable_infer\": True,\n",
    "            \"desalt_index\":\"data/genome/indexes/human\",\n",
    "            \"accessions_map\": \"data/simulated/accessions_map_ens.csv\",\n",
    "    },\n",
    "    # \"SIM_ANN\": {\"ref\": \"data/genome/GRCh38.p12_genomic.fna\" ,\n",
    "    #         \"reads\": \"data/simulated/reads_sim/reads_sim.fa\" ,\n",
    "    #        \"gtf\": \"data/genome/annotations/gencode.v34.chr_patch_hapl_scaff.annotation.gtf\" ,\n",
    "    #         \"bed\":\"data/genome/annotations/bed/gencode.v34.annotation.bed\",\n",
    "    #         \"desalt_gtf\": \"data/genome/annotations/desalt_gtf/gencode.v34.annotation.bed\",\n",
    "    #         \"ont\": False,\n",
    "    #         \"isoseq\": False,\n",
    "    #         \"disable_infer\": True,\n",
    "    #         \"desalt_index\":\"data/genome/indexes/human\",\n",
    "    #         \"accessions_map\": \"data/simulated/accessions_map_sim.csv\",\n",
    "    # },\n",
    "#     \"SIM_NIC\": {\"ref\": \"data/genome/GRCh38.p13_genomic.fna\" ,\n",
    "#             \"reads\": \"data/simulated/reads_nic.fa\" ,\n",
    "#             \"gtf\": \"data/genome/annotations/gencode.v34.annotation.gff3\" ,\n",
    "#             \"bed\":\"data/genome/annotations/bed/gencode.v34.annotation.bed\",\n",
    "#             \"ont\": False,\n",
    "#             \"isoseq\": False,\n",
    "#             \"disable_infer\": True,\n",
    "#             \"desalt_index\":\"data/genome/indexes/humanIndex\",   \n",
    "#             \"accessions_map\": \"data/simulated/accessions_map_nic.csv\",\n",
    "\n",
    "#     },\n",
    "#     \"SIRV\": {\"ref\": \"data/genome/SIRV_isoforms_multi-fasta_170612a.fasta\",\n",
    "#             \"reads\": \"data/SIRV/SIRV_processed.fastq\",\n",
    "#             \"gtf\": \"data/genome/annotations/SIRV_isoforms_Lot00141_multi-fasta-annotation_C_170612a.gtf\",\n",
    "#             \"bed\":\"data/genome/annotations/bed/SIRV_isoforms_Lot00141_multi-fasta-annotation_C_170612a.bed\",\n",
    "#             \"desalt_gtf\": \"data/genome/annotations/desalt_gtf/SIRV_isoforms_Lot00141_multi-fasta-annotation_C_170612a.gtf\",\n",
    "#             \"ont\": True,\n",
    "#             \"isoseq\": False,\n",
    "#             \"disable_infer\": False,\n",
    "#             \"desalt_index\":\"data/genome/indexes/sirv\",\n",
    "#     },\n",
    "    # \"DROS\": {\"ref\": \"data/genome/DROS.BDGP6.28.all.fa\",\n",
    "    #         \"reads\": \"data/DROS_processed.fastq\",\n",
    "    #         \"gtf\": \"data/genome/annotations/Drosophila_melanogaster.BDGP6.28.100.gtf\",\n",
    "    #         \"bed\":\"data/genome/annotations/bed/Drosophila_melanogaster.BDGP6.28.100.bed\",\n",
    "    #         \"desalt_gtf\": \"data/genome/annotations/desalt_gtf/Drosophila_melanogaster.BDGP6.28.100.gtf\",\n",
    "    #         \"ont\": True,\n",
    "    #         \"isoseq\": False,\n",
    "    #         \"disable_infer\": True,\n",
    "    #         \"desalt_index\":\"data/genome/indexes/da\",   \n",
    "    # },\n",
    "    # \"ALZ\": {\"ref\": \"data/genome/GRCh38.p12.genome.fa\" ,\n",
    "    #         \"reads\": \"data/simulated/reads_ens.fa\" ,\n",
    "    #         \"gtf\": \"data/genome/annotations/gencode.v34.chr_patch_hapl_scaff.annotation.gtf\" ,\n",
    "    #         \"bed\": \"data/genome/annotations/bed/gencode.v34.annotation.bed\",\n",
    "    #         \"desalt_gtf\": \"data/genome/annotations/desalt_gtf/gencode.v34.annotation.bed\",\n",
    "    #         \"ont\": False,\n",
    "    #         \"isoseq\": True,\n",
    "    #         \"disable_infer\": True,\n",
    "    #         \"desalt_index\":\"data/genome/indexes/human\",\n",
    "    # }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ccfcf7-030c-4b47-b90b-da69c910eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time(directory):\n",
    "    subdirs = next(os.walk(directory))[1]\n",
    "    times = []\n",
    "    tools = ['uLTRA', 'uLTRA_mm2', 'minimap2', 'minimap2_GTF', 'deSALT', 'deSALT_GTF', 'graphmap2', 'graphmap2_GTF']\n",
    "    for tool in tools:\n",
    "        time = json.load(open(os.path.join(directory, tool, 'stats.json'), 'r'))['total_time']\n",
    "        times.append(time)\n",
    "    plt.clf()\n",
    "    plt.bar(tools, times)\n",
    "    plt.xlabel(\"Tool\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.savefig(\"time.png\")\n",
    "        \n",
    "    \n",
    "def real_data_pipeline(dataset):\n",
    "    splice_annotations_csvs = []\n",
    "    category_stats = {\"uLTRA\" :{}, \"uLTRA_mm2\" :{}, \"minimap2\" :{}, \"minimap2_GTF\" :{}, \"deSALT\" :{}, \"deSALT_GTF\" :{}}\n",
    "    \n",
    "    ref = dataset[\"ref\"]\n",
    "    gtf = dataset[\"gtf\"]\n",
    "    bed = dataset[\"bed\"]\n",
    "    desalt_gtf = dataset[\"desalt_gtf\"]\n",
    "    reads = dataset[\"reads\"]\n",
    "    ont = dataset[\"ont\"]\n",
    "    isoseq = dataset[\"isoseq\"]\n",
    "    disable_infer = dataset[\"disable_infer\"]\n",
    "    desalt_index = dataset[\"desalt_index\"]\n",
    "\n",
    "\n",
    "    #ultra\n",
    "    toolname = \"uLTRA\"\n",
    "    args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, True, desalt_index)\n",
    "    general_stats = ultra(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "\n",
    "    #ultra_mm2\n",
    "    toolname = \"uLTRA_mm2\"\n",
    "    args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    general_mm_stats = ultra(args)\n",
    "    sam_file = os.path.join(args.outfolder, \"reads.sam\")\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, sam_file, toolname) \n",
    "    splice_annotations_csvs.append(os.path.join(args.outfolder, \"splice_annotations.csv\"))\n",
    "\n",
    "    # mm2\n",
    "    toolname = \"minimap2\"\n",
    "    args = create_args(ref, gtf, None, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    minimap2(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    splice_annotations_csvs.append(os.path.join(args.outfolder, \"splice_annotations.csv\"))\n",
    "    \n",
    "    # mm2_GTF\n",
    "    toolname = \"minimap2_GTF\"\n",
    "    args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    minimap2(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    splice_annotations_csvs.append(os.path.join(args.outfolder, \"splice_annotations.csv\"))\n",
    "\n",
    "    # deSALT\n",
    "    toolname = \"deSALT\"\n",
    "    args = create_args(ref, gtf, bed, None, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    deSALT(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    \n",
    "    # deSALT_GTF\n",
    "    toolname = \"deSALT_GTF\"\n",
    "    args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    deSALT(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    splice_annotations_csvs.append(os.path.join(args.outfolder, \"splice_annotations.csv\"))\n",
    "\n",
    "\n",
    "    # # graphmap2\n",
    "    # toolname = \"graphmap2\"\n",
    "    # args = create_args(ref, None, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    # graphmap2(args)\n",
    "    # args.gtf = gtf\n",
    "    # category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    # splice_annotations_csvs.append(os.path.join(args.outfolder, \"splice_annotations.csv\"))\n",
    "    \n",
    "    # graphmap2_GTF\n",
    "    # toolname = \"graphmap2_GTF\"\n",
    "    # args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    # graphmap2(args)\n",
    "    # category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    # splice_annotations_csvs.append(os.path.join(args.outfolder, \"splice_annotations.csv\"))\n",
    "\n",
    "    # Final outputs\n",
    "    # concordance\n",
    "    combined_splice_annotations = pd.concat([pd.read_csv(f) for f in splice_annotations_csvs])\n",
    "    combined_splice_annotations_path = os.path.join(\"output\", dataset_name, \"splice_annotations.csv\")\n",
    "    combined_splice_annotations.to_csv(combined_splice_annotations_path, index=False, encoding='utf-8-sig')\n",
    "    get_diff_loc_reads.get_diff_loc_reads(args, combined_splice_annotations_path, os.path.join(\"output\", dataset_name))\n",
    "    \n",
    "    # extra category\n",
    "    plots.category_plot(category_stats, os.path.join(\"output\", dataset_name))\n",
    "    plot_time(os.path.join(\"output\", dataset_name))\n",
    "    \n",
    "    \n",
    "def sim_data_pipeline(dataset):\n",
    "   \n",
    "    results_per_read_csvs = []\n",
    "    correctness_per_exon_size_csvs = []\n",
    "    category_stats = {\"uLTRA\" :{}, \"uLTRA_mm2\" :{}, \"minimap2\" :{}, \"minimap2_GTF\" :{}, \"deSALT\" :{}, \"deSALT_GTF\" :{}}\n",
    "    \n",
    "    ref = dataset[\"ref\"]\n",
    "    gtf = dataset[\"gtf\"]\n",
    "    bed = dataset[\"bed\"]\n",
    "    desalt_gtf = dataset[\"desalt_gtf\"]\n",
    "    reads = dataset[\"reads\"]\n",
    "    ont = dataset[\"ont\"]\n",
    "    isoseq = dataset[\"isoseq\"]\n",
    "    disable_infer = dataset[\"disable_infer\"]\n",
    "    desalt_index = dataset[\"desalt_index\"]\n",
    "    accessions_map = dataset[\"accessions_map\"]\n",
    "\n",
    "    # ultra\n",
    "    toolname = \"uLTRA\"\n",
    "    args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, True, desalt_index)\n",
    "    general_stats = ultra(args)\n",
    "    if(not (os.path.exists(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\")) \n",
    "            and os.path.exists(os.path.join(args.outfolder, \"results_per_read.csv\")))):\n",
    "        evaluate_exons.evaluate_sim_reads(args, accessions_map)\n",
    "    correctness_per_exon_size_csvs.append(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\"))\n",
    "    results_per_read_csvs.append(os.path.join(args.outfolder, \"results_per_read.csv\"))`\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    \n",
    "    #ultra_mm2\n",
    "    toolname = \"uLTRA_mm2\"\n",
    "    args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    general_mm_stats = ultra(args)\n",
    "    sam_file = os.path.join(args.outfolder, \"reads.sam\")\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, sam_file, toolname) \n",
    "    if(not (os.path.exists(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\")) \n",
    "            and os.path.exists(os.path.join(args.outfolder, \"results_per_read.csv\")))):\n",
    "        evaluate_exons.evaluate_sim_reads(args, accessions_map)\n",
    "    correctness_per_exon_size_csvs.append(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\"))\n",
    "    results_per_read_csvs.append(os.path.join(args.outfolder, \"results_per_read.csv\"))\n",
    "    \n",
    "    #minimap2\n",
    "    toolname = \"minimap2\"\n",
    "    args = create_args(ref, gtf, None, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    minimap2(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    if(not (os.path.exists(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\")) \n",
    "            and os.path.exists(os.path.join(args.outfolder, \"results_per_read.csv\")))):\n",
    "        evaluate_exons.evaluate_sim_reads(args, accessions_map)\n",
    "    correctness_per_exon_size_csvs.append(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\"))\n",
    "    results_per_read_csvs.append(os.path.join(args.outfolder, \"results_per_read.csv\"))\n",
    "    \n",
    "    # mm2_GTF\n",
    "    toolname = \"minimap2_GTF\"\n",
    "    args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    minimap2(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    if(not (os.path.exists(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\")) \n",
    "            and os.path.exists(os.path.join(args.outfolder, \"results_per_read.csv\")))):\n",
    "        evaluate_exons.evaluate_sim_reads(args, accessions_map)\n",
    "    correctness_per_exon_size_csvs.append(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\"))\n",
    "    results_per_read_csvs.append(os.path.join(args.outfolder, \"results_per_read.csv\"))\n",
    "    \n",
    "    # desalt\n",
    "    toolname = \"deSALT\"\n",
    "    args = create_args(ref, gtf, bed, None, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    deSALT(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    if(not (os.path.exists(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\")) \n",
    "            and os.path.exists(os.path.join(args.outfolder, \"results_per_read.csv\")))):\n",
    "        evaluate_exons.evaluate_sim_reads(args, accessions_map)\n",
    "    correctness_per_exon_size_csvs.append(os.path.join(args.outfolder, \"correctness_per_exon_size.csv\"))\n",
    "    results_per_read_csvs.append(os.path.join(args.outfolder, \"results_per_read.csv\"))`\n",
    "    \n",
    "    # desalt_GTf\n",
    "    toolname = \"deSALT_GTF\"\n",
    "    args = create_args(ref, gtf, bed, desalt_gtf, reads, ont, isoseq, disable_infer, dataset_name, toolname, False, desalt_index)\n",
    "    deSALT(args)\n",
    "    category_stats[toolname] = evaluate_splice_annotations.evaluate_splice_annotations(args, os.path.join(args.outfolder, \"reads.sam\"), toolname) \n",
    "    if(not (os.path.exists(os.path.join(args.outfolder, \"agreement_per_exon_size_biological.csv\")) \n",
    "            and os.path.exists(os.path.join(args.outfolder, \"results_per_read_biological.csv\")))):\n",
    "        evaluate_exons.evaluate_sim_reads(args, accessions_map)\n",
    "    correctness_per_exon_size_csvs.append(os.path.join(args.outfolder, \"agreement_per_exon_size_biological.csv\"))\n",
    "    results_per_read_csvs.append(os.path.join(args.outfolder, \"results_per_read_biological.csv\"))\n",
    "    \n",
    "    # Final outputs\n",
    "    # Final outputs\n",
    "    results_per_read = pd.concat([pd.read_csv(f) for f in results_per_read_csvs])\n",
    "    results_per_read_path = os.path.join(\"output\", dataset_name, \"results_per_read.csv\")\n",
    "    results_per_read.to_csv(results_per_read_path, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    correctness_per_exon_size = pd.concat([pd.read_csv(f) for f in correctness_per_exon_size_csvs])\n",
    "    correctness_per_exon_size_path = os.path.join(\"output\", dataset_name, \"correctness_per_exon_size.csv\")\n",
    "    correctness_per_exon_size.to_csv(correctness_per_exon_size_path, index=False, encoding='utf-8-sig')\n",
    "   \n",
    "    # extra category\n",
    "    plots.category_plot(category_stats, os.path.join(\"output\", dataset_name))\n",
    "    plots.alignment_accuracy_plot(results_per_read_path, os.path.join(\"output\", dataset_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317efa89-3cfb-47c0-b978-c2a75754aaa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset_name in dataset_dict.keys():\n",
    "    dataset = dataset_dict[dataset_name]\n",
    "    is_real = dataset[\"ont\"] or dataset[\"isoseq\"] # if the dataset is real\n",
    "    if is_real:\n",
    "        real_data_pipeline(dataset)\n",
    "    else:\n",
    "        sim_data_pipeline(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
